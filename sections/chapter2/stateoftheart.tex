

\section{Camera modelling fundamentals in computer vision}

\subsection{Pinhole camera}
\subsubsection{intrinsic parameters}

\subsection{Lens distortion correction and camera calibration}
\subsubsection{distortion parameters}
\subsubsection{extrinsic parameters}

\subsection{Fish-eye camera}


\section{Solutions for stereoscopy}

\subsection{Stereopsis}
Depth perception may arise from a variety of depth cues, most of them related to a scene viewed by a single eye. These go from motion parallax to simple size differential, nothing that pinhole camera cannot reproduce, as much as its implementation in computer graphics. What stereo imagery must focus on is delivering additional binocular cues, like stereopsis and convergence. Stereoscopy uses two images of the same scene obtained from slightly different angles to do the trick. This is not just an intuitive guess: binocular parallax is what our brain uses to extract image disparities and do its best to estimate real objects distances in our sight. Being able to reproduce those disparities tricks the viewer to be able to percieve depth (stereopsis) and consequently make his extraocular muscles contract (convergence). At the same time, disparity information can be used for stereo computer vision and extract depth data as well.\\
(TO EDIT briefly explain how stereoptis work and why)\\
Assuming null distortion, a stereo vision system can be described with the following parameters:
\begin{itemize}
\item the two camera intrinsic matrices, their aperture and focus distance;
\item their position and orientation in space defined by origin and optical axis; their separation and yaw angle is mostly relevant for stereoscopic effect;
\item epipolar lines and plane, given a chosen point in space within both frustrums(*).
\end{itemize}
Through these we will briefly describe most used configurations for stereo vision today and what they involve in terms of perception and practical implementation.

\subsection{Stereo rig configurations}
The following configurations imply the assumption that the two cameras have same declared characteristics and each of them has no distortion (or has been calibrated). A good rule of thumb in stereoscopic movie filming is to choose camera aperture and focal length first, depending on the distance where the scene should be on focus which will in turn correspond to the portion of the scene perceived on the screen level. Camera separation can then be decided proportional to eye separation (referred to as IPD, inter-pupillary distance) and around 1/30 of the distance at which they should be displayed. This is why, ideally, stereoscopic content should be designed specifically for the size of the screen on which they will be shown and expected audience position.

\subsubsection{parallel cameras}
The most intuitive and used approach in stereoscopic computer graphics (for virtual cameras) is to place two cameras next to each other with their optical axis parallel. Line connecting camera origins (baseline) and epipolar lines are perfectly parallel. In theory this is enough to give the horizontal disparity we are looking for, but is also where imaging devices meet their limit. Stereo pairs, which are the pairs of points of the scene whose projection is visible in both images, can be found only in a portion of each frame, which corresponds to the portion of the scene visible by both cameras. This means that left-most part and right-most part of left and right camera respectively won’t offer useful stereo information. This is much like what happens at the edges of our eyesight, angles that only one eye at a time is able to see. Although those areas don’t contribute to stereopsis, it is still what enables us to get such high FOV and the perception of being in the middle of the scene. It is not a case that this happens to be one of the key points to achieve good immersion in VR simulations.\\
More than merely collect horizontal disparities, we want to have also control over the natural convergence effect of the eyes. For a spherical imaging device such as the human eye, the disparity is expressed in terms of visual angle. A flat sensor of classic cameras (obeying to pinhole model) can instead be seen as the limiting form of a spherical sensor with an infinite radius of curvature (ref*). This means that an in-ward rotation of the sensors might do the trick. However, for the parallel stereo baseline to be maintained, its equivalent would be a translation of the cameras parallel to sensor plane or alternatively a translation of the image on the display, if this happens to be a flat surface. By shifting displayed frames on the screen we can control depth of the objects perceived by the observer, that will be seen in front (negative parallax) or behind the screen (positive parallax). The screen plane is coincident with what is called zero-parallax plane. With no shifting, zero-parallax plane is placed at infinite and every object is perceived in front of the screen.\\
A practical problem with shifting the displayed images on screen is that part of each half-image falls off the boundaries of the screen, therefore losing precious FOV the more we move objects behind the screen. This is also way parallel-type is much used in computer graphics, where it is not a problem to customize virtual camera parameters to cover a higher portion of the scene from the beginning with zero cost. Also, having most of the scene behind the display is preferred situation in stereoscopy. Having much of the scene too far from the zero-parallax plane causes a disruption of the natural synergy between vergence and accomodation for most users (since objects are not where each eye focuses on). Different reasons make behind-the-screen philosophy a preference: something coming out from the screen more likely will cross its boundaries and will screw up occlusion perception; targets far away are more likely to be on focus together with the screen than closer ones.

\subsubsection{off-axis cameras}
A real camera can be hacked so that it gets all advantages and simplicity of parallel-type configuration without giving away FOV. Instead of shifting display frame afterwards or translating cameras taking the risk of unnatural results, we can play with sensor/lens alignment. By shifting the sensor in respect to the lens, we can decide the actual convergence angle exploiting all available sensor area. What happens is that camera frustrum is no longer symmetric and allows us to cover different areas of the scene without rotating the lens/sensor. This method is also called shifted-lens or skewed-frustrum stereo and is the ideal way of capturing/reproducing stereo pairs. Unfortunately, such vision systems are not very diffused and are difficoult to realize in practice due to the high precision needed. Moreover, vergence angle is something to be decided in advance, leaving each device tied to a specific scene and screen target. This is why existing systems of this kind provide adjustable lens/sensor shift and are much more expensive and therefore uncommon.\\
While this effect is hard to achieve in practice, it can be expressed by the pin-hole model (by means of Principal Point Offset). However, in 3D graphics world (where cameras could be entirely customized) still many applications don’t go beyond the standard virtual camera model, which as we will see is described by focus point, view and up vector and FOV. A virtual camera matrix (in the specific, its “projection” component) is determined by those parameters even though it is possible to entirely customize it to get the desired skewed-frustrum.

\subsubsection{toed-in cameras}
Optical axis in toed-in configuration converge and baseline intersects with the two camera projection planes (epipolar lines are no longer parallel, but their orientation depend on the stereo pair). As we introduced the convergence problem in parallel-type configuration, we mentioned that rotation of the sensor is a way to go. In reality, this is the pursued option in most cases. The reason is obviously its easy and low-cost implementation, plus a decent approximation of what an off-axis setup would offer. The whole FOV of each camera is indeed preserved and the yaw of each camera can be easily adjusted at capture time. The fundamental problem is that while toe-in stereo makes sense intuitively (as our eyes rotate inwards when we focus on nearby objects), this does not correspond to how 3D is shown. Captured frames are in fact projected into a screen and not differently to each retina. The whole assumption that the image will be displayed on a display orthogonal to viewer’s direction (as happens for single camera captures) is still considered valid in 3D movies where there are actually two viewing directions projected into the same plane. This leads to the effect known as “keystoning”: artificial vertical disparity is introduced, increasing towards the edges, causing a breakdown of the 3D illusion. Even in less severe scenarios where brain is flexible enough to adapt, it will eventually lead to eye strain, not to mention the already present accomodation/convergence conflict. Since this effect is noticeable at the edges, strategies like artificially reduce the amount of eye separation and keep objects, and therefore the viewer’s eyes, in the center of the screen may address the symptoms but not the cause.\\
Note that this setup is still the good for stereo computer vision, which does not really focus about getting acceptable stereopsis and convergence but simply working with as much stereo pairs as possible (how pairs are mapped into depth information is only a matter of calibration). This is also the preferred setup for stereoscopic imaging in photography (where the subject is captured only once and under controlled circumstances) and is still acceptable for video capturing under the assumption that eyes will be most of the time focused on the center of the screen and a good compromise between camera tilt and distance from subject is reached.

\subsection{Stereo camera calibration for stereo vision}
Analysed stereo rig models assume geometric constraints are respected, which is an ideal case. Though human eye can still tolerate slight alignment approximation, stereo computer vision requires precision to obtain reliable data. Stereo calibration process is similar to single camera calibration: it involves more steps, includes getting both intrinsic parameters from each camera and extrinsics for the stereo setup. This way camera position and pose can be considered in the equation. The aim is to make corresponding epipolar lines in image pairs be parallel to horizontal direction. We won’t analyse all proposed solutions to this problem as this falls beyond the scope of this text. We will however put some effort in considering its relationship with previously mentioned configurations. In order to avoid heavy computation in image undistortion, a recent study (*2011) exploits the relationship between the general-type unconstrainted configuration (which is uncalibrated rig) to its virtual parallel-type equivalent. It is worth mentioning the proposed algorithm avoids completely complex calculations based on epipolar lines or fundamental matrix and also skips single camera calibration phase (true?). The resulting images appear to be considered as if they were captured by a parallel stereo camera rig with its own optical axis sharing the same origin: raw frame points are re-projected into a new plane and after rectification there is no residue of keystoning, at the cost of reducing perceived FOV. However, since original capture is issued by a toed-in configuration, it cannot be considered a real parallel equivalent; only a portion of rectified frame will contain informational content, leaving black areas within view angles reached by the parallel configuration only.\\
Stereo calibration is not really necessary where we have reasonable control on stereo rig build quality and goal is obtaining stereopsis only. It is however mandatory for stereo vision algorithms to work properly, which sure is the first step to take to push this work further. Keep in mind that real-time re-sampling of the image is not always feasible and performance is vital to achieve needed FPS if this has to do with VR/AR applications. Also consider that effort in getting physical camera alignment right is never too much: FOV is precious and could considerably deteriorate both object detection and user experience.

\subsection{From standard to fish-eye stereo}


\section{Modelling first person view in VR}

\subsection{Virtual camera: projection and view matrix principles}

\subsection{Head model: IPD and ETN parameters}

\subsection{Rendering pipeline for HMDs}

\subsection{Tackling lag: Oculus Rift timewarping}










